{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d0648-3ef0-4b6f-830b-46fe4670baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once\n",
    "!pip install torch torchvision pytorchvideo opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594a288-012f-4856-ac0e-38c2f35e73e1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b886af-2217-40ec-a228-365c3e011bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\kaiav/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n",
      "C:\\Users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Initialize model\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True) #else some imports won't work for some reason\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.transforms import Compose, Resize, Normalize\n",
    "\n",
    "from typing import Dict\n",
    "import json\n",
    "import urllib\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50ed15-48ba-42f7-912b-a0d685515f1b",
   "metadata": {},
   "source": [
    "## CSV/DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af447fe5-cb5c-4606-b810-b104031ea611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate clip names found.\n",
      "All filenames start with 'clip_'.\n",
      "\n",
      "       clip_name label                clip_path\n",
      "1  clip_0001.mp4     0  raw_clips\\clip_0001.mp4\n",
      "2  clip_0002.mp4     1  raw_clips\\clip_0002.mp4\n",
      "4  clip_0004.mp4     1  raw_clips\\clip_0004.mp4\n",
      "5  clip_0005.mp4     1  raw_clips\\clip_0005.mp4\n",
      "8  clip_0008.mp4     0  raw_clips\\clip_0008.mp4\n"
     ]
    }
   ],
   "source": [
    "# Import, clean up csv file, add clip paths\n",
    "csv_filename = 'labels.csv'\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# print( 'Unique labels: ', df['label'].unique() )                               # print label values (should only be 0, 1, 2)\n",
    "# print( 'Original shape: ', df.shape)                                           # print original shape\n",
    "# print( 'Data preview:\\n', df.head() )                                          # preview data\n",
    "if df['clip_name'].duplicated().any():                                         # check for duplicates\n",
    "    print(\"There are duplicate clip names.\")\n",
    "else:\n",
    "    print(\"No duplicate clip names found.\")\n",
    "if not df['clip_name'].str.startswith(\"clip_\").all():                          # validate all filenames start with 'clip_'\n",
    "    print(\"Some filenames do not start with 'clip_':\")\n",
    "    print(df[~df['clip_name'].str.startswith(\"clip_\")])                        # print invalid rows, if any\n",
    "else:\n",
    "    print(\"All filenames start with 'clip_'.\")    \n",
    "print()\n",
    "df['label'] = df['label'].astype(str).str.strip().str.lower()                  # convert to string, strip whitespaces, convert to lowercase\n",
    "df = df.drop(df[df['label'] == '2'].index)                                     # prune invalid/void pass/dribble scenarios\n",
    "# print( 'Unique labels: ', df['label'].unique() )                               # print label values (should only be 0/1)\n",
    "# print( 'New shape: ', df.shape )                                               # print pruned df shape\n",
    "\n",
    "df['clip_path'] = df['clip_name'].apply(lambda x: os.path.join('raw_clips', x))# add column for full clip path\n",
    "print( df.head() )                                                             # preview dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa23c0-5cfa-465e-a1ad-fd27fabc4c49",
   "metadata": {},
   "source": [
    "## Input Transform for SlowFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "363ae499-ad0c-4a5e-976c-c5da6fb165dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/facebookresearch_pytorchvideo_slowfast.ipynb \n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "slowfast_alpha = 4\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0243fee-e53a-4782-b442-89bb6436e9fa",
   "metadata": {},
   "source": [
    "## VideoDataSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160f2c1e-6c83-41b1-b726-b488ca509cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom DataSet object\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_frame, transform=transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_frame (pd.DataFrame): DataFrame with columns ['clip_name', 'label', 'clip_path']\n",
    "            transform (callable, optional): Transform to be applied to video frames\n",
    "        Returns: frames and label for given clip\n",
    "        \"\"\"\n",
    "        self.data_frame = data_frame\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get clip path and label\n",
    "        clip_path = self.data_frame.iloc[idx]['clip_path']\n",
    "        label = self.data_frame.iloc[idx]['label']\n",
    "        label = int(label)\n",
    "\n",
    "        # Initialize an EncodedVideo helper class and load the video\n",
    "        video = EncodedVideo.from_path(clip_path)\n",
    "        video_data = video.get_clip(0, 1) # 0 to 1 sec\n",
    "        \n",
    "        # Apply a transform to normalize the video input\n",
    "        video_data = transform(video_data)\n",
    "        \n",
    "        # Move the frames to the desired device\n",
    "        frames = video_data[\"video\"]\n",
    "        frames = [i.to(device)[None, ...] for i in frames]\n",
    "        frames = [i.squeeze(0) for i in frames]\n",
    "        \n",
    "        # print('frames: \\n', frames)\n",
    "        \n",
    "        return frames, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2abe44-9097-49bb-b7d6-e19f5e86e653",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864a482f-5c17-4cb8-8910-7894f6e5e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and set up DataLoaders\n",
    "\n",
    "# Split data frames into train/test/validation\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)  # 70% train\n",
    "test_df, val_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 15% test 15% validation\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = VideoDataset(train_df, transform=transform)\n",
    "test_dataset = VideoDataset(test_df, transform=transform)\n",
    "val_dataset = VideoDataset(val_df, transform=transform)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c2f26-abea-4430-bef3-f4145737ede7",
   "metadata": {},
   "source": [
    "## Sinlge Clip Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d229020-4bb4-43ff-8d93-240ffca0f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "# (model already initialized in imports)\n",
    "device = \"cpu\"\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8486ec31-f00c-4419-8989-88e8aac98095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinetics labels mapping\n",
    "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "json_filename = \"kinetics_classnames.json\"\n",
    "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
    "except: urllib.request.urlretrieve(json_url, json_filename)\n",
    "with open(json_filename, \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fa2bcb7-85cb-418c-8da1-9513874a3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 8, 256, 256])\n",
      "Top 5 predicted labels: air drumming, abseiling\n"
     ]
    }
   ],
   "source": [
    "# Model inference\n",
    "train_iter = iter(train_loader)\n",
    "frames, label = next(train_iter)\n",
    "# print(f\"Frames shape: {frames.shape}, Label: {label}\")\n",
    "print( frames[0].shape )\n",
    "# frames[0].squeeze(0) \n",
    "# print( frames[0].shape )\n",
    "new_model = torch.nn.Sequential(model, nn.Linear(400, 2))\n",
    "preds = new_model(frames.copy())\n",
    "# preds = model(frames.copy())\n",
    "\n",
    "# Get the predicted classes\n",
    "post_act = torch.nn.Softmax(dim=1)\n",
    "preds = post_act(preds)\n",
    "pred_classes = preds.topk(k=2).indices[0]\n",
    "\n",
    "# Map the predicted classes to the label names\n",
    "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9de79dd2-ebc0-4ea8-9cfb-3fe4ca8f7ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7daa5-2157-4daf-b12f-ecc64b2fc48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE BASELINE MODEL PERFORMANCE (test set)\n",
    "# model.eval()\n",
    "for name, param in new_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print( name, param.shape )\n",
    "#freeze feature extraction layers / turn off requires_grad \n",
    "#replace final layer with a binary classification layer (single neuron for pass/dribble)\n",
    "#train for 3 epochs\n",
    "#test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd3586-5a69-4ec5-9256-104c778838d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE-TUNE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556202f-e4a0-4edb-a013-b7ff37b7da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE FINE-TUNED MODEL PERFORMANCE (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f40474-c461-4663-b261-a984bf8d20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUTE FINE-TUNED MODEL GENERALIZATION PERFORMANCE (validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824958ba-1874-4c7d-80f9-d8468b2c118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a single clip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
