{"cells":[{"cell_type":"code","execution_count":null,"id":"0b0d0648-3ef0-4b6f-830b-46fe4670baa5","metadata":{"id":"0b0d0648-3ef0-4b6f-830b-46fe4670baa5"},"outputs":[],"source":["# Run once if needed\n","!pip install torch torchvision pytorchvideo opencv-python scikit-learn"]},{"cell_type":"code","source":["# Mount drive to access labels.csv and raw_clips in drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"692XRoBgDpyh","executionInfo":{"status":"ok","timestamp":1733794677389,"user_tz":300,"elapsed":31371,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}},"outputId":"cc74d1b5-7b83-47b2-f4a1-f87987d2e267"},"id":"692XRoBgDpyh","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"9594a288-012f-4856-ac0e-38c2f35e73e1","metadata":{"id":"9594a288-012f-4856-ac0e-38c2f35e73e1"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"id":"d0b886af-2217-40ec-a228-365c3e011bd3","metadata":{"id":"d0b886af-2217-40ec-a228-365c3e011bd3"},"outputs":[],"source":["import torch\n","# Initialize model\n","model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True) #else some imports won't work for some reason\n","import torch.nn as nn\n","import torch.optim as optim\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","import cv2\n","import numpy as np\n","import os\n","from torchvision.transforms import Compose, Resize, Normalize\n","from tqdm import tqdm\n","\n","from typing import Dict\n","import json\n","import urllib\n","from torchvision.transforms import Compose, Lambda\n","from torchvision.transforms._transforms_video import (\n","    CenterCropVideo,\n","    NormalizeVideo,\n",")\n","from pytorchvideo.data.encoded_video import EncodedVideo\n","from pytorchvideo.transforms import (\n","    ApplyTransformToKey,\n","    ShortSideScale,\n","    UniformTemporalSubsample,\n","    UniformCropVideo\n",")"]},{"cell_type":"code","source":["# Check if a GPU is available\n","if torch.cuda.is_available():\n","    gpu_name = torch.cuda.get_device_name(0)\n","    print(f\"GPU is available: {gpu_name}\")\n","else:\n","    print(\"GPU is not available.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMLFLCr6CV0_","executionInfo":{"status":"ok","timestamp":1733794732008,"user_tz":300,"elapsed":202,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}},"outputId":"1a34d405-6f91-4e37-e353-3df41e05609c"},"id":"IMLFLCr6CV0_","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available: Tesla T4\n"]}]},{"cell_type":"markdown","id":"7c50ed15-48ba-42f7-912b-a0d685515f1b","metadata":{"id":"7c50ed15-48ba-42f7-912b-a0d685515f1b"},"source":["## CSV/DF"]},{"cell_type":"code","source":["# Unzip clips from drive, store in temp colab virtual storage\n","zip_file_path = \"/content/drive/Shareddrives/Deep Learning K.A.K.A. Project 3/essential/raw_clips_kai.zip\"\n","extract_to_dir = \"/content/raw_clips\"\n","\n","!unzip \"{zip_file_path}\" -d \"{extract_to_dir}\""],"metadata":{"id":"doyqSTTZO1a2"},"id":"doyqSTTZO1a2","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":9,"id":"af447fe5-cb5c-4606-b810-b104031ea611","metadata":{"id":"af447fe5-cb5c-4606-b810-b104031ea611","outputId":"d56857ba-f9af-4645-9074-a1658737a5c3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733796383352,"user_tz":300,"elapsed":982,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["No duplicate clip names found.\n","All filenames start with 'clip_'.\n","\n","       clip_name label                                   clip_path\n","1  clip_0001.mp4     0  /content/raw_clips/raw_clips/clip_0001.mp4\n","2  clip_0002.mp4     1  /content/raw_clips/raw_clips/clip_0002.mp4\n","4  clip_0004.mp4     1  /content/raw_clips/raw_clips/clip_0004.mp4\n","5  clip_0005.mp4     1  /content/raw_clips/raw_clips/clip_0005.mp4\n","8  clip_0008.mp4     0  /content/raw_clips/raw_clips/clip_0008.mp4\n"]}],"source":["# Import, clean up csv file, add clip paths\n","csv_filename = '/content/drive/Shareddrives/Deep Learning K.A.K.A. Project 3/essential/labels_kai.csv'\n","df = pd.read_csv(csv_filename)\n","\n","# print( 'Unique labels: ', df['label'].unique() )                               # print label values (should only be 0, 1, 2)\n","# print( 'Original shape: ', df.shape)                                           # print original shape\n","# print( 'Data preview:\\n', df.head() )                                          # preview data\n","if df['clip_name'].duplicated().any():                                         # check for duplicates\n","    print(\"There are duplicate clip names.\")\n","else:\n","    print(\"No duplicate clip names found.\")\n","if not df['clip_name'].str.startswith(\"clip_\").all():                          # validate all filenames start with 'clip_'\n","    print(\"Some filenames do not start with 'clip_':\")\n","    print(df[~df['clip_name'].str.startswith(\"clip_\")])                        # print invalid rows, if any\n","else:\n","    print(\"All filenames start with 'clip_'.\")\n","print()\n","df['label'] = df['label'].astype(str).str.strip().str.lower()                  # convert to string, strip whitespaces, convert to lowercase\n","df = df.drop(df[df['label'] == '2'].index)                                     # prune invalid/void pass/dribble scenarios\n","# print( 'Unique labels: ', df['label'].unique() )                               # print label values (should only be 0/1)\n","# print( 'New shape: ', df.shape )                                               # print pruned df shape\n","\n","df['clip_path'] = df['clip_name'].apply(lambda x: os.path.join('/content/raw_clips/raw_clips', x))# add column for full clip path\n","print( df.head() )                                                             # preview dataframe"]},{"cell_type":"markdown","id":"0caa23c0-5cfa-465e-a1ad-fd27fabc4c49","metadata":{"id":"0caa23c0-5cfa-465e-a1ad-fd27fabc4c49"},"source":["## Input Transform for SlowFast"]},{"cell_type":"code","execution_count":10,"id":"363ae499-ad0c-4a5e-976c-c5da6fb165dc","metadata":{"id":"363ae499-ad0c-4a5e-976c-c5da6fb165dc","executionInfo":{"status":"ok","timestamp":1733796404183,"user_tz":300,"elapsed":249,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}}},"outputs":[],"source":["# Code from: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/facebookresearch_pytorchvideo_slowfast.ipynb\n","side_size = 256\n","mean = [0.45, 0.45, 0.45]\n","std = [0.225, 0.225, 0.225]\n","crop_size = 256\n","num_frames = 32\n","slowfast_alpha = 4\n","\n","class PackPathway(torch.nn.Module):\n","    \"\"\"\n","    Transform for converting video frames as a list of tensors.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, frames: torch.Tensor):\n","        fast_pathway = frames\n","        # Perform temporal sampling from the fast pathway.\n","        slow_pathway = torch.index_select(\n","            frames,\n","            1,\n","            torch.linspace(\n","                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n","            ).long(),\n","        )\n","        frame_list = [slow_pathway, fast_pathway]\n","        return frame_list\n","\n","transform =  ApplyTransformToKey(\n","    key=\"video\",\n","    transform=Compose(\n","        [\n","            UniformTemporalSubsample(num_frames),\n","            Lambda(lambda x: x/255.0),\n","            NormalizeVideo(mean, std),\n","            ShortSideScale(\n","                size=side_size\n","            ),\n","            CenterCropVideo(crop_size),\n","            PackPathway()\n","        ]\n","    ),\n",")"]},{"cell_type":"markdown","id":"b0243fee-e53a-4782-b442-89bb6436e9fa","metadata":{"id":"b0243fee-e53a-4782-b442-89bb6436e9fa"},"source":["## VideoDataSet"]},{"cell_type":"code","execution_count":11,"id":"160f2c1e-6c83-41b1-b726-b488ca509cd5","metadata":{"id":"160f2c1e-6c83-41b1-b726-b488ca509cd5","executionInfo":{"status":"ok","timestamp":1733796408943,"user_tz":300,"elapsed":197,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}}},"outputs":[],"source":["# Create custom DataSet object\n","class VideoDataset(Dataset):\n","    def __init__(self, data_frame, transform=transform):\n","        \"\"\"\n","        Args:\n","            data_frame (pd.DataFrame): DataFrame with columns ['clip_name', 'label', 'clip_path']\n","            transform (callable, optional): Transform to be applied to video frames\n","        Returns: frame_pathways and label for given clip\n","        \"\"\"\n","        self.data_frame = data_frame\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data_frame)\n","\n","    def __getitem__(self, idx):\n","        # Get clip path and label\n","        clip_path = self.data_frame.iloc[idx]['clip_path']\n","        label = self.data_frame.iloc[idx]['label']\n","        label = torch.tensor( int(label) )\n","\n","        # Initialize an EncodedVideo helper class and load the video\n","        video = EncodedVideo.from_path(clip_path)\n","        video_data = video.get_clip(0, 1) # 0 to 1 sec\n","\n","        # Apply a transform to normalize the video input\n","        video_data = transform(video_data)\n","\n","        # Move the frame_pathways and label to the desired device\n","        frame_pathways = video_data[\"video\"]\n","        frame_pathways = [i.to(device) for i in frame_pathways]\n","        label = label.to(device)\n","\n","        return frame_pathways, label"]},{"cell_type":"markdown","id":"bb2abe44-9097-49bb-b7d6-e19f5e86e653","metadata":{"id":"bb2abe44-9097-49bb-b7d6-e19f5e86e653"},"source":["## DataLoaders"]},{"cell_type":"code","execution_count":12,"id":"864a482f-5c17-4cb8-8910-7894f6e5e088","metadata":{"id":"864a482f-5c17-4cb8-8910-7894f6e5e088","executionInfo":{"status":"ok","timestamp":1733796412514,"user_tz":300,"elapsed":242,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}}},"outputs":[],"source":["# Split data and set up DataLoaders\n","\n","# Split data frames into train/test/validation\n","train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)  # 70% train\n","test_df, val_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 15% test 15% validation\n","\n","# Create Dataset objects\n","train_dataset = VideoDataset(train_df, transform=transform)\n","test_dataset = VideoDataset(test_df, transform=transform)\n","val_dataset = VideoDataset(val_df, transform=transform)\n","\n","# Create DataLoader objects\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"]},{"cell_type":"markdown","id":"05c4be40-0c89-4a95-8643-9ebd48446c3c","metadata":{"id":"05c4be40-0c89-4a95-8643-9ebd48446c3c"},"source":["## Model Setup"]},{"cell_type":"code","execution_count":13,"id":"35e083c5-d19c-4a35-b6e3-ba43d90580b7","metadata":{"id":"35e083c5-d19c-4a35-b6e3-ba43d90580b7","executionInfo":{"status":"ok","timestamp":1733796442017,"user_tz":300,"elapsed":724,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}}},"outputs":[],"source":["# model = slowfast_r50 imported in Imports section\n","new_fc = nn.Linear(400, 2)\n","new_model = torch.nn.Sequential(model, new_fc) # this is the final model we will use to classify pass vs dribble clips\n","softmax = torch.nn.Softmax(dim=1) #post activation function from raw logits from model output\n","\n","device = \"cuda\"\n","new_model = new_model.to(device)"]},{"cell_type":"markdown","id":"de0c2f26-abea-4430-bef3-f4145737ede7","metadata":{"id":"de0c2f26-abea-4430-bef3-f4145737ede7"},"source":["## Sinlge Batch Inference"]},{"cell_type":"code","execution_count":14,"id":"6fa2bcb7-85cb-418c-8da1-9513874a3dae","metadata":{"id":"6fa2bcb7-85cb-418c-8da1-9513874a3dae","outputId":"367eb4e8-5d9a-48ae-99a5-22d51a2b6efb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733796462843,"user_tz":300,"elapsed":14456,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch size: 8\n","Clip frames shape: torch.Size([8, 3, 8, 256, 256])\n","Labels: tensor([0, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')\n","Predicted labels: tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"]}],"source":["# Get a batch of clip and label pairs\n","test_iter = iter(test_loader)\n","frame_pathways, labels = next(test_iter) #frame_pathways holds the slow and fast pathway tensors, so its a list of 2 tensors, each one has all the frames for the batch\n","print(f\"Batch size: {frame_pathways[0].shape[0]}\\nClip frames shape: {frame_pathways[0].shape}\\nLabels: {labels}\")\n","\n","#Get predictions\n","new_model.eval()\n","preds = new_model(frame_pathways.copy())\n","preds = softmax(preds)\n","preds = torch.argmax(preds, dim=1)\n","\n","print(f\"Predicted labels: {preds}\")"]},{"cell_type":"markdown","id":"94aacc4d-1596-4d6e-b80e-6886660ed95b","metadata":{"id":"94aacc4d-1596-4d6e-b80e-6886660ed95b"},"source":["## Baseline performance\n","Plan\n","1) test baseline performance\n","2) fine-tune model\n","3) evaluate fine-tuned model performance"]},{"cell_type":"code","execution_count":15,"id":"b6dbff33-0f5f-42c3-b597-b4762252c0d1","metadata":{"id":"b6dbff33-0f5f-42c3-b597-b4762252c0d1","executionInfo":{"status":"ok","timestamp":1733796475189,"user_tz":300,"elapsed":205,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}}},"outputs":[],"source":["def test_accuracy():\n","    new_model.eval()\n","    total = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for frame_pathways, labels in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n","            # Get model predictions\n","            outputs = new_model(frame_pathways)\n","\n","            # Apply softmax if not part of the model's last layer\n","            probabilities = softmax(outputs)\n","\n","            # Get predicted class (highest probability)\n","            predicted_classes = torch.argmax(probabilities, dim=1)\n","\n","            # Update the total and correct counts\n","            total += labels.size(0)  # Total samples in this batch\n","            correct += (predicted_classes == labels).sum().item()  # Correct predictions\n","\n","    # Calculate accuracy\n","    accuracy = correct / total\n","    print(f\"Accuracy: {accuracy * 100:.2f}%\")"]},{"cell_type":"code","execution_count":16,"id":"5e5549b5-adc4-400f-a2b0-922e818d430a","metadata":{"id":"5e5549b5-adc4-400f-a2b0-922e818d430a","outputId":"1d5a4dad-159e-4564-a474-2b67599df551","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733796779930,"user_tz":300,"elapsed":284173,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 22/22 [04:44<00:00, 12.91s/batch]"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 52.60%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["test_accuracy()"]},{"cell_type":"markdown","id":"b5ebc029-7b38-4dd4-ae1b-265833637d88","metadata":{"id":"b5ebc029-7b38-4dd4-ae1b-265833637d88"},"source":["## Fine-tuning"]},{"cell_type":"code","execution_count":17,"id":"fd7c21ed-5846-4709-8b57-261ba2d5d66a","metadata":{"id":"fd7c21ed-5846-4709-8b57-261ba2d5d66a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733798082651,"user_tz":300,"elapsed":1302728,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}},"outputId":"de7fe6c7-5dac-4f39-a606-3a903d259f50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/1], Step [10/101], Loss: 1.0474\n","Epoch [1/1], Step [20/101], Loss: 0.6646\n","Epoch [1/1], Step [30/101], Loss: 0.8132\n","Epoch [1/1], Step [40/101], Loss: 0.6568\n","Epoch [1/1], Step [50/101], Loss: 0.6105\n","Epoch [1/1], Step [60/101], Loss: 0.5950\n","Epoch [1/1], Step [70/101], Loss: 0.5535\n","Epoch [1/1], Step [80/101], Loss: 0.6248\n","Epoch [1/1], Step [90/101], Loss: 0.7566\n","Epoch [1/1], Step [100/101], Loss: 1.0276\n","Epoch [1/1], Average Loss: 0.7037\n","Training complete!\n"]}],"source":["# Freeze base layers, leaving only the last two FC layers for fine-tuning\n","for param in model.parameters():\n","    param.requires_grad = False\n","for param in new_fc.parameters():\n","    param.requires_grad = True\n","# Set up loss function, optimizer, etc\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(new_fc.parameters(), lr=0.001)\n","num_epochs = 1\n","\n","for epoch in range(num_epochs):\n","    new_model.train()\n","    running_loss = 0.0\n","\n","    for batch_idx, (frame_pathways, labels) in enumerate(train_loader):\n","        # Forward pass\n","        outputs = new_model(frame_pathways)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        if (batch_idx + 1) % 10 == 0:  # Print every 10 batches\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n","\n","    # Print average loss for the epoch\n","    avg_loss = running_loss / len(train_loader)\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n","\n","print(\"Training complete!\")"]},{"cell_type":"markdown","id":"280797e9-10c1-4f03-b26f-3f5feaa3cc34","metadata":{"id":"280797e9-10c1-4f03-b26f-3f5feaa3cc34"},"source":["## Fine-tuned performance"]},{"cell_type":"code","execution_count":18,"id":"898f04ab-239c-4b31-beeb-42028e9c71cd","metadata":{"id":"898f04ab-239c-4b31-beeb-42028e9c71cd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733798382028,"user_tz":300,"elapsed":286401,"user":{"displayName":"Kai Avni","userId":"05489117014026366516"}},"outputId":"fafcaf60-4ee0-404f-9c07-8a5deca3249c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 22/22 [04:46<00:00, 13.01s/batch]"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 67.63%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["test_accuracy()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"},"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}