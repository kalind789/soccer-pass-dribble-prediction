{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b0d0648-3ef0-4b6f-830b-46fe4670baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once\n",
    "# !pip install torch torchvision pytorchvideo opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594a288-012f-4856-ac0e-38c2f35e73e1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0b886af-2217-40ec-a228-365c3e011bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\kaiav/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Initialize model\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True) #else some imports won't work for some reason\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.transforms import Compose, Resize, Normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Dict\n",
    "import json\n",
    "import urllib\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50ed15-48ba-42f7-912b-a0d685515f1b",
   "metadata": {},
   "source": [
    "## CSV/DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af447fe5-cb5c-4606-b810-b104031ea611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate clip names found.\n",
      "All filenames start with 'clip_'.\n",
      "\n",
      "       clip_name label                clip_path\n",
      "1  clip_0001.mp4     0  raw_clips\\clip_0001.mp4\n",
      "2  clip_0002.mp4     1  raw_clips\\clip_0002.mp4\n",
      "4  clip_0004.mp4     1  raw_clips\\clip_0004.mp4\n",
      "5  clip_0005.mp4     1  raw_clips\\clip_0005.mp4\n",
      "8  clip_0008.mp4     0  raw_clips\\clip_0008.mp4\n"
     ]
    }
   ],
   "source": [
    "# Import, clean up csv file, add clip paths\n",
    "csv_filename = 'labels.csv'\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# print( 'Unique labels: ', df['label'].unique() )                               # print label values (should only be 0, 1, 2)\n",
    "# print( 'Original shape: ', df.shape)                                           # print original shape\n",
    "# print( 'Data preview:\\n', df.head() )                                          # preview data\n",
    "if df['clip_name'].duplicated().any():                                         # check for duplicates\n",
    "    print(\"There are duplicate clip names.\")\n",
    "else:\n",
    "    print(\"No duplicate clip names found.\")\n",
    "if not df['clip_name'].str.startswith(\"clip_\").all():                          # validate all filenames start with 'clip_'\n",
    "    print(\"Some filenames do not start with 'clip_':\")\n",
    "    print(df[~df['clip_name'].str.startswith(\"clip_\")])                        # print invalid rows, if any\n",
    "else:\n",
    "    print(\"All filenames start with 'clip_'.\")    \n",
    "print()\n",
    "df['label'] = df['label'].astype(str).str.strip().str.lower()                  # convert to string, strip whitespaces, convert to lowercase\n",
    "df = df.drop(df[df['label'] == '2'].index)                                     # prune invalid/void pass/dribble scenarios\n",
    "# print( 'Unique labels: ', df['label'].unique() )                               # print label values (should only be 0/1)\n",
    "# print( 'New shape: ', df.shape )                                               # print pruned df shape\n",
    "\n",
    "df['clip_path'] = df['clip_name'].apply(lambda x: os.path.join('raw_clips', x))# add column for full clip path\n",
    "print( df.head() )                                                             # preview dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa23c0-5cfa-465e-a1ad-fd27fabc4c49",
   "metadata": {},
   "source": [
    "## Input Transform for SlowFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "363ae499-ad0c-4a5e-976c-c5da6fb165dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/facebookresearch_pytorchvideo_slowfast.ipynb \n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "slowfast_alpha = 4\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0243fee-e53a-4782-b442-89bb6436e9fa",
   "metadata": {},
   "source": [
    "## VideoDataSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "160f2c1e-6c83-41b1-b726-b488ca509cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom DataSet object\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_frame, transform=transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_frame (pd.DataFrame): DataFrame with columns ['clip_name', 'label', 'clip_path']\n",
    "            transform (callable, optional): Transform to be applied to video frames\n",
    "        Returns: frame_pathways and label for given clip\n",
    "        \"\"\"\n",
    "        self.data_frame = data_frame\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get clip path and label\n",
    "        clip_path = self.data_frame.iloc[idx]['clip_path']\n",
    "        label = self.data_frame.iloc[idx]['label']\n",
    "        label = torch.tensor( int(label) )\n",
    "\n",
    "        # Initialize an EncodedVideo helper class and load the video\n",
    "        video = EncodedVideo.from_path(clip_path)\n",
    "        video_data = video.get_clip(0, 1) # 0 to 1 sec\n",
    "        \n",
    "        # Apply a transform to normalize the video input\n",
    "        video_data = transform(video_data)\n",
    "        \n",
    "        # Move the frame_pathways and label to the desired device\n",
    "        frame_pathways = video_data[\"video\"]\n",
    "        frame_pathways = [i.to(device) for i in frame_pathways]\n",
    "        label = label.to(device)\n",
    "        \n",
    "        return frame_pathways, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2abe44-9097-49bb-b7d6-e19f5e86e653",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "864a482f-5c17-4cb8-8910-7894f6e5e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and set up DataLoaders\n",
    "\n",
    "# Split data frames into train/test/validation\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)  # 70% train\n",
    "test_df, val_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 15% test 15% validation\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = VideoDataset(train_df, transform=transform)\n",
    "test_dataset = VideoDataset(test_df, transform=transform)\n",
    "val_dataset = VideoDataset(val_df, transform=transform)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4be40-0c89-4a95-8643-9ebd48446c3c",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35e083c5-d19c-4a35-b6e3-ba43d90580b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = slowfast_r50 imported in Imports section\n",
    "new_fc = nn.Linear(400, 2)\n",
    "new_model = torch.nn.Sequential(model, new_fc) # this is the final model we will use to classify pass vs dribble clips\n",
    "softmax = torch.nn.Softmax(dim=1) #post activation function from raw logits from model output\n",
    "\n",
    "device = \"cpu\"\n",
    "new_model = new_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c2f26-abea-4430-bef3-f4145737ede7",
   "metadata": {},
   "source": [
    "## Sinlge Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fa2bcb7-85cb-418c-8da1-9513874a3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Clip frames shape: torch.Size([8, 3, 8, 256, 256])\n",
      "Labels: tensor([0, 1, 0, 0, 1, 1, 1, 0])\n",
      "Predicted labels: tensor([1, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of clip and label pairs\n",
    "test_iter = iter(test_loader)\n",
    "frame_pathways, labels = next(test_iter) #frame_pathways holds the slow and fast pathway tensors, so its a list of 2 tensors, each one has all the frames for the batch\n",
    "print(f\"Batch size: {frame_pathways[0].shape[0]}\\nClip frames shape: {frame_pathways[0].shape}\\nLabels: {labels}\")\n",
    "\n",
    "#Get predictions\n",
    "new_model.eval()\n",
    "preds = new_model(frame_pathways.copy())\n",
    "preds = softmax(preds)\n",
    "preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "print(f\"Predicted labels: {preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aacc4d-1596-4d6e-b80e-6886660ed95b",
   "metadata": {},
   "source": [
    "## Baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6dbff33-0f5f-42c3-b597-b4762252c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy():\n",
    "    new_model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for frame_pathways, labels in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n",
    "            # Get model predictions\n",
    "            outputs = new_model(frame_pathways)\n",
    "            \n",
    "            # Apply softmax if not part of the model's last layer\n",
    "            probabilities = softmax(outputs)\n",
    "            \n",
    "            # Get predicted class (highest probability)\n",
    "            predicted_classes = torch.argmax(probabilities, dim=1)\n",
    "            \n",
    "            # Update the total and correct counts\n",
    "            total += labels.size(0)  # Total samples in this batch\n",
    "            correct += (predicted_classes == labels).sum().item()  # Correct predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e5549b5-adc4-400f-a2b0-922e818d430a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████| 22/22 [10:13<00:00, 27.90s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ebc029-7b38-4dd4-ae1b-265833637d88",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c21ed-5846-4709-8b57-261ba2d5d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base layers, leaving only the last two FC layers for fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in new_fc.parameters():\n",
    "    param.requires_grad = True\n",
    "# Set up loss function, optimizer, etc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(new_fc.parameters(), lr=0.001)\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    new_model.train()\n",
    "    running_loss = 0.0 \n",
    "\n",
    "    for batch_idx, (frame_pathways, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = new_model(frame_pathways)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280797e9-10c1-4f03-b26f-3f5feaa3cc34",
   "metadata": {},
   "source": [
    "## Fine-tuned performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898f04ab-239c-4b31-beeb-42028e9c71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63d24c-5dab-4fc5-9e67-0a5e62bcb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan\n",
    "# 1) fine-tune model a little\n",
    "# 2) test baseline performance\n",
    "# 3) finish fine-tuning with entire dataset\n",
    "# 4) evaluate fine-tuned model performance (with the test set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
