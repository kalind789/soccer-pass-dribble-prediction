{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b0d0648-3ef0-4b6f-830b-46fe4670baa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (0.20.1+cpu)\n",
      "Requirement already satisfied: pytorchvideo in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (0.1.5)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: fvcore in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from pytorchvideo) (0.1.5.post20221221)\n",
      "Requirement already satisfied: av in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from pytorchvideo) (13.1.0)\n",
      "Requirement already satisfied: parameterized in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from pytorchvideo) (0.9.0)\n",
      "Requirement already satisfied: iopath in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from pytorchvideo) (0.1.10)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from fvcore->pytorchvideo) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from fvcore->pytorchvideo) (6.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from fvcore->pytorchvideo) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from fvcore->pytorchvideo) (2.5.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from fvcore->pytorchvideo) (0.9.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from iopath->pytorchvideo) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from portalocker->iopath->pytorchvideo) (305.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from tqdm->fvcore->pytorchvideo) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Run once\n",
    "!pip install torch torchvision pytorchvideo opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594a288-012f-4856-ac0e-38c2f35e73e1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b886af-2217-40ec-a228-365c3e011bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\kaiav/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n",
      "C:\\Users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Initialize model\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True) #else some imports won't work for some reason\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.transforms import Compose, Resize, Normalize\n",
    "\n",
    "from typing import Dict\n",
    "import json\n",
    "import urllib\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50ed15-48ba-42f7-912b-a0d685515f1b",
   "metadata": {},
   "source": [
    "## CSV/DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af447fe5-cb5c-4606-b810-b104031ea611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels:  [2 0 1]\n",
      "Original shape:  (2395, 2)\n",
      "No duplicate clip names found.\n",
      "All filenames start with 'clip_'.\n",
      "\n",
      "Unique labels:  ['0' '1']\n",
      "New shape:  (1154, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import, clean up csv file, add clip paths\n",
    "csv_filename = 'labels.csv'\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "print( 'Unique labels: ', df['label'].unique() )                               # print label values (should only be 0, 1, 2)\n",
    "print( 'Original shape: ', df.shape)                                           # print original shape\n",
    "# print( 'Data preview:\\n', df.head() )                                          # preview data\n",
    "if df['clip_name'].duplicated().any():                                         # check for duplicates\n",
    "    print(\"There are duplicate clip names.\")\n",
    "else:\n",
    "    print(\"No duplicate clip names found.\")\n",
    "if not df['clip_name'].str.startswith(\"clip_\").all():                          # validate all filenames start with 'clip_'\n",
    "    print(\"Some filenames do not start with 'clip_':\")\n",
    "    print(df[~df['clip_name'].str.startswith(\"clip_\")])                        # print invalid rows, if any\n",
    "else:\n",
    "    print(\"All filenames start with 'clip_'.\")    \n",
    "print()\n",
    "df['label'] = df['label'].astype(str).str.strip().str.lower()                  # convert to string, strip whitespaces, convert to lowercase\n",
    "df = df.drop(df[df['label'] == '2'].index)                                     # prune invalid/void pass/dribble scenarios\n",
    "print( 'Unique labels: ', df['label'].unique() )                               # print label values (should only be 0/1)\n",
    "print( 'New shape: ', df.shape )                                               # print pruned df shape\n",
    "\n",
    "df['clip_path'] = df['clip_name'].apply(lambda x: os.path.join('raw_clips', x))# add column for full clip path\n",
    "# print( df.head() )                                                             # preview dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa23c0-5cfa-465e-a1ad-fd27fabc4c49",
   "metadata": {},
   "source": [
    "## Input Transform for SlowFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363ae499-ad0c-4a5e-976c-c5da6fb165dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/facebookresearch_pytorchvideo_slowfast.ipynb \n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "slowfast_alpha = 4\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0243fee-e53a-4782-b442-89bb6436e9fa",
   "metadata": {},
   "source": [
    "## VideoDataSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "160f2c1e-6c83-41b1-b726-b488ca509cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom DataSet object\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_frame, transform=transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_frame (pd.DataFrame): DataFrame with columns ['clip_name', 'label', 'clip_path']\n",
    "            transform (callable, optional): Transform to be applied to video frames\n",
    "        Returns: frames and label for given clip\n",
    "        \"\"\"\n",
    "        self.data_frame = data_frame\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get clip path and label\n",
    "        clip_path = self.data_frame.iloc[idx]['clip_path']\n",
    "        label = self.data_frame.iloc[idx]['label']\n",
    "        label = int(label)\n",
    "\n",
    "        # Initialize an EncodedVideo helper class and load the video\n",
    "        video = EncodedVideo.from_path(clip_path)\n",
    "        video_data = video.get_clip(0, 1) # 0 to 1 sec\n",
    "        \n",
    "        # Apply a transform to normalize the video input\n",
    "        video_data = transform(video_data)\n",
    "        \n",
    "        # Move the frames to the desired device\n",
    "        frames = video_data[\"video\"]\n",
    "        frames = [i.to(device)[None, ...] for i in frames]\n",
    "        \n",
    "        # print('frames: \\n', frames)\n",
    "        \n",
    "        return frames, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2abe44-9097-49bb-b7d6-e19f5e86e653",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864a482f-5c17-4cb8-8910-7894f6e5e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and set up DataLoaders\n",
    "\n",
    "# Split data frames into train/test/validation\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)  # 70% train\n",
    "test_df, val_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 15% test 15% validation\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = VideoDataset(train_df, transform=transform)\n",
    "test_dataset = VideoDataset(test_df, transform=transform)\n",
    "val_dataset = VideoDataset(val_df, transform=transform)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c2f26-abea-4430-bef3-f4145737ede7",
   "metadata": {},
   "source": [
    "## Sinlge Clip Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d229020-4bb4-43ff-8d93-240ffca0f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "# (model already initialized in imports)\n",
    "device = \"cpu\"\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8486ec31-f00c-4419-8989-88e8aac98095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinetics labels mapping\n",
    "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "json_filename = \"kinetics_classnames.json\"\n",
    "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
    "except: urllib.request.urlretrieve(json_url, json_filename)\n",
    "with open(json_filename, \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fa2bcb7-85cb-418c-8da1-9513874a3dae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "input for MultiPathWayWithFuse needs to be a list of tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m frames, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_iter)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(f\"Frames shape: {frames.shape}, Label: {label}\")\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get the predicted classes\u001b[39;00m\n\u001b[0;32m      9\u001b[0m post_act \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\\pytorchvideo\\models\\net.py:43\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[1;32m---> 43\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\\pytorchvideo\\models\\net.py:108\u001b[0m, in \u001b[0;36mMultiPathWayWithFuse.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: List[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    109\u001b[0m         x, \u001b[38;5;28mlist\u001b[39m\n\u001b[0;32m    110\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput for MultiPathWayWithFuse needs to be a list of tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace:\n\u001b[0;32m    112\u001b[0m         x_out \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[1;31mAssertionError\u001b[0m: input for MultiPathWayWithFuse needs to be a list of tensors"
     ]
    }
   ],
   "source": [
    "# Model inference\n",
    "train_iter = iter(train_loader)\n",
    "frames, label = next(train_iter)\n",
    "# print(f\"Frames shape: {frames.shape}, Label: {label}\")\n",
    "print( frames[0].shape )\n",
    "\n",
    "preds = model(frames)\n",
    "\n",
    "# Get the predicted classes\n",
    "post_act = torch.nn.Softmax(dim=1)\n",
    "preds = post_act(preds)\n",
    "pred_classes = preds.topk(k=5).indices[0]\n",
    "\n",
    "# Map the predicted classes to the label names\n",
    "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3244971-8674-4ca5-a58a-1ce9549ef1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 8, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print( frames[0].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7daa5-2157-4daf-b12f-ecc64b2fc48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE BASELINE MODEL PERFORMANCE (test set)\n",
    "# model.eval()\n",
    "\n",
    "#freeze feature extraction layers / turn off requires_grad \n",
    "#replace final layer with a binary classification layer (single neuron for pass/dribble)\n",
    "#train for 3 epochs\n",
    "#test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd3586-5a69-4ec5-9256-104c778838d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE-TUNE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556202f-e4a0-4edb-a013-b7ff37b7da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE FINE-TUNED MODEL PERFORMANCE (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f40474-c461-4663-b261-a984bf8d20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUTE FINE-TUNED MODEL GENERALIZATION PERFORMANCE (validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824958ba-1874-4c7d-80f9-d8468b2c118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a single clip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
