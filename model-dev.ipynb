{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b0d0648-3ef0-4b6f-830b-46fe4670baa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (0.20.1+cpu)\n",
      "Requirement already satisfied: pytorchvideo in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (0.1.5)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: fvcore in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from pytorchvideo) (0.1.5.post20221221)\n",
      "Requirement already satisfied: av in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from pytorchvideo) (13.1.0)\n",
      "Requirement already satisfied: parameterized in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from pytorchvideo) (0.9.0)\n",
      "Requirement already satisfied: iopath in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from pytorchvideo) (0.1.10)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from fvcore->pytorchvideo) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from fvcore->pytorchvideo) (6.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from fvcore->pytorchvideo) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from fvcore->pytorchvideo) (2.5.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from fvcore->pytorchvideo) (0.9.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from iopath->pytorchvideo) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from portalocker->iopath->pytorchvideo) (305.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages (from tqdm->fvcore->pytorchvideo) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Run once\n",
    "!pip install torch torchvision pytorchvideo opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594a288-012f-4856-ac0e-38c2f35e73e1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b886af-2217-40ec-a228-365c3e011bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\kaiav/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n",
      "C:\\Users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kaiav\\anaconda3\\envs\\deep_learning_project\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Initialize model\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True) #else some imports won't work for some reason\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.transforms import Compose, Resize, Normalize\n",
    "\n",
    "from typing import Dict\n",
    "import json\n",
    "import urllib\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50ed15-48ba-42f7-912b-a0d685515f1b",
   "metadata": {},
   "source": [
    "## CSV/DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af447fe5-cb5c-4606-b810-b104031ea611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate clip names found.\n",
      "All filenames start with 'clip_'.\n",
      "\n",
      "       clip_name label                clip_path\n",
      "1  clip_0001.mp4     0  raw_clips\\clip_0001.mp4\n",
      "2  clip_0002.mp4     1  raw_clips\\clip_0002.mp4\n",
      "4  clip_0004.mp4     1  raw_clips\\clip_0004.mp4\n",
      "5  clip_0005.mp4     1  raw_clips\\clip_0005.mp4\n",
      "8  clip_0008.mp4     0  raw_clips\\clip_0008.mp4\n"
     ]
    }
   ],
   "source": [
    "# Import, clean up csv file, add clip paths\n",
    "csv_filename = 'labels.csv'\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# print( 'Unique labels: ', df['label'].unique() )                               # print label values (should only be 0, 1, 2)\n",
    "# print( 'Original shape: ', df.shape)                                           # print original shape\n",
    "# print( 'Data preview:\\n', df.head() )                                          # preview data\n",
    "if df['clip_name'].duplicated().any():                                         # check for duplicates\n",
    "    print(\"There are duplicate clip names.\")\n",
    "else:\n",
    "    print(\"No duplicate clip names found.\")\n",
    "if not df['clip_name'].str.startswith(\"clip_\").all():                          # validate all filenames start with 'clip_'\n",
    "    print(\"Some filenames do not start with 'clip_':\")\n",
    "    print(df[~df['clip_name'].str.startswith(\"clip_\")])                        # print invalid rows, if any\n",
    "else:\n",
    "    print(\"All filenames start with 'clip_'.\")    \n",
    "print()\n",
    "df['label'] = df['label'].astype(str).str.strip().str.lower()                  # convert to string, strip whitespaces, convert to lowercase\n",
    "df = df.drop(df[df['label'] == '2'].index)                                     # prune invalid/void pass/dribble scenarios\n",
    "# print( 'Unique labels: ', df['label'].unique() )                               # print label values (should only be 0/1)\n",
    "# print( 'New shape: ', df.shape )                                               # print pruned df shape\n",
    "\n",
    "df['clip_path'] = df['clip_name'].apply(lambda x: os.path.join('raw_clips', x))# add column for full clip path\n",
    "print( df.head() )                                                             # preview dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa23c0-5cfa-465e-a1ad-fd27fabc4c49",
   "metadata": {},
   "source": [
    "## Input Transform for SlowFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363ae499-ad0c-4a5e-976c-c5da6fb165dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/facebookresearch_pytorchvideo_slowfast.ipynb \n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "slowfast_alpha = 4\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0243fee-e53a-4782-b442-89bb6436e9fa",
   "metadata": {},
   "source": [
    "## VideoDataSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "160f2c1e-6c83-41b1-b726-b488ca509cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom DataSet object\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_frame, transform=transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_frame (pd.DataFrame): DataFrame with columns ['clip_name', 'label', 'clip_path']\n",
    "            transform (callable, optional): Transform to be applied to video frames\n",
    "        Returns: frames and label for given clip\n",
    "        \"\"\"\n",
    "        self.data_frame = data_frame\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get clip path and label\n",
    "        clip_path = self.data_frame.iloc[idx]['clip_path']\n",
    "        label = self.data_frame.iloc[idx]['label']\n",
    "        label = int(label)\n",
    "\n",
    "        # Initialize an EncodedVideo helper class and load the video\n",
    "        video = EncodedVideo.from_path(clip_path)\n",
    "        video_data = video.get_clip(0, 1) # 0 to 1 sec\n",
    "        \n",
    "        # Apply a transform to normalize the video input\n",
    "        video_data = transform(video_data)\n",
    "        \n",
    "        # Move the frames to the desired device\n",
    "        frames = video_data[\"video\"]\n",
    "        frames = [i.to(device)[None, ...] for i in frames]\n",
    "        frames = [i.squeeze(0) for i in frames]\n",
    "        \n",
    "        clip = frames\n",
    "        \n",
    "        return clip, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2abe44-9097-49bb-b7d6-e19f5e86e653",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864a482f-5c17-4cb8-8910-7894f6e5e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and set up DataLoaders\n",
    "\n",
    "# Split data frames into train/test/validation\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)  # 70% train\n",
    "test_df, val_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 15% test 15% validation\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = VideoDataset(train_df, transform=transform)\n",
    "test_dataset = VideoDataset(test_df, transform=transform)\n",
    "val_dataset = VideoDataset(val_df, transform=transform)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4be40-0c89-4a95-8643-9ebd48446c3c",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e083c5-d19c-4a35-b6e3-ba43d90580b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (original slowfast_r50 model already initialized in imports)\n",
    "device = \"cpu\"\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "new_model = torch.nn.Sequential(model, nn.Linear(400, 2)) # this is the final model we will use to classify pass vs dribble clips\n",
    "post_act = torch.nn.Softmax(dim=1) #post activation function from raw logits from model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c2f26-abea-4430-bef3-f4145737ede7",
   "metadata": {},
   "source": [
    "## Sinlge Clip Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fa2bcb7-85cb-418c-8da1-9513874a3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 5\n",
      "Clip frames shape: torch.Size([5, 3, 8, 256, 256])\n",
      "Labels: tensor([0, 0, 0, 1, 0])\n",
      "Predicted labels: tensor([0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of clip and label pairs\n",
    "train_iter = iter(train_loader)\n",
    "frame_pathways, labels = next(train_iter) #frame_pathways holds the slow and fast pathway tensors, so its a list of 2 tensors, each one has all the frames for the batch\n",
    "print(f\"Batch size: {frame_pathways[0].shape[0]}\\nClip frames shape: {frame_pathways[0].shape}\\nLabels: {labels}\")\n",
    "\n",
    "#Get predictions\n",
    "preds = new_model(frame_pathways.copy())\n",
    "preds = post_act(preds)\n",
    "preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "print(f\"Predicted labels: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c21ed-5846-4709-8b57-261ba2d5d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune model a little bit\n",
    "# model.train() ?\n",
    "# batches of 8, 5 epochs\n",
    "# define criterion binary cross entropy loss\n",
    "# update params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63d24c-5dab-4fc5-9e67-0a5e62bcb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan\n",
    "# 1) fine-tune model a little\n",
    "# 2) test baseline performance\n",
    "# 3) finish fine-tuning with entire dataset\n",
    "# 4) evaluate fine-tuned model performance (with the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b7daa5-2157-4daf-b12f-ecc64b2fc48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE BASELINE MODEL PERFORMANCE (test set)\n",
    "# model.eval()\n",
    "# for name, param in new_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print( name, param.shape )\n",
    "\n",
    "#freeze feature extraction layers / turn off requires_grad \n",
    "#replace final layer with a binary classification layer (single neuron for pass/dribble)\n",
    "#train for 3 epochs\n",
    "#test accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
